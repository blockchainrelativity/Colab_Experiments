{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blockchainrelativity/Colab_Experiments/blob/main/meeting_transcripts_w_speaker_names.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACobbJnIR_ni"
      },
      "source": [
        "Notes on usage:\n",
        "\n",
        "- Make sure to [change runtime to GPU](https://www.tutorialspoint.com/google_colab/google_colab_using_free_gpu.htm).\n",
        "- The transcript will be saved in Files, which you can find in the menu on the left.\n",
        "- Change the number of speakers below if different from two.\n",
        "- Pick a bigger model if you want more accuracy and a smaller model if you want the program to run faster ([more info](https://github.com/openai/whisper#available-models-and-languages)).\n",
        "- If you know the language being spoken is English, then change language to 'English' as this improves performance.\n",
        "\n",
        "\n",
        "High level overview of what's happening here:\n",
        "\n",
        "\n",
        "1.   I'm using Open AI's Whisper model to seperate audio into segments and generate transcripts.\n",
        "2.   I'm then generating speaker embeddings for each segments.\n",
        "3.   Then I'm using agglomerative clustering on the embeddings to identify the speaker for each segment.   \n",
        "\n",
        "Let me know if I can make it better!\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BE SURE TO SET THESE VARIABLES in the appropriate cell:\n",
        "\n",
        "# input_file_path\n",
        "# num_speakers =  #@param {type:\"integer\"}\n",
        "# language = '' #@param ['any', 'English']\n",
        "# model_size"
      ],
      "metadata": {
        "id": "KLbmoToO8qUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxYITgmGfR9p"
      },
      "outputs": [],
      "source": [
        "# upload audio file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "path = next(iter(uploaded))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OBoHXgfywaG0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfd7bfd7-3b65-4e22-be8c-19c30526d688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# *****Set chosen file path here******\n",
        "input_file_path = \"/content/drive/MyDrive/Meet Recordings/Emurgo Hackathon - Moving Parts (2023-05-21 15:05 GMT-5) (1)\""
      ],
      "metadata": {
        "id": "ZDfamVh4zBiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihF90TRgg3bL"
      },
      "outputs": [],
      "source": [
        "# Creating directory for export of audio files into wav format\n",
        "!mkdir transcripts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use when source is MP4. Convert MP4 into wav for model\n",
        "\n",
        "!pip install moviepy\n",
        "\n",
        "from moviepy.editor import AudioFileClip\n",
        "\n",
        "# Set the paths of the input and output files\n",
        "# input_file_path = \"audio.mp4\"\n",
        "output_file_path = \"transcripts/audio.wav\"\n",
        "\n",
        "# Load the video file\n",
        "video = AudioFileClip(input_file_path)\n",
        "\n",
        "# Extract the audio and save it as a WAV file\n",
        "audio = video.write_audiofile(output_file_path, codec='pcm_s16le')\n",
        "# , ffmpeg_params=['-num_channels','1']\n"
      ],
      "metadata": {
        "id": "iirotnqipaC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-2PqfF0qlW6"
      },
      "outputs": [],
      "source": [
        "# # Use when source is m4a\n",
        "\n",
        "# !pip install pydub\n",
        "# from pydub import AudioSegment\n",
        "\n",
        "# # Set the input and output file paths\n",
        "# input_file = \"audio1039974750.m4a\"\n",
        "# output_file = \"transcripts/audio.wav\"\n",
        "\n",
        "# # Read the input file\n",
        "# audio = AudioSegment.from_file(input_file, format=\"m4a\")\n",
        "\n",
        "# # Export the audio as a WAV file\n",
        "# audio.export(output_file, format=\"wav\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buGt4moR5Mac"
      },
      "outputs": [],
      "source": [
        "num_speakers = 5 #@param {type:\"integer\"}\n",
        "\n",
        "language = 'English' #@param ['any', 'English']\n",
        "\n",
        "model_size = 'medium' #@param ['tiny', 'base', 'small', 'medium', 'large']\n",
        "\n",
        "\n",
        "model_name = model_size\n",
        "if language == 'English' and model_size != 'medium':\n",
        "  model_name += '.en'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TesEj6GHh2Yx",
        "outputId": "0adf7495-bfe0-4fac-9f86-6818cc295368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UTF-8\n"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "print(locale.getpreferredencoding())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # ***Alternative to moviepy***\n",
        "# # pip install cloudconvert. This will convert many file types.\n",
        "\n",
        "# import cloudconvert\n",
        "\n",
        "# cloudconvert.configure(api_key = 'API_KEY', sandbox = False)\n",
        "\n",
        "# import CloudConvert from 'cloudconvert';\n",
        "\n",
        "# const cloudConvert = new CloudConvert('api_key');\n",
        "\n",
        "# let job = await cloudConvert.jobs.create({\n",
        "#     \"tasks\": {\n",
        "#         \"import-mp4\": {\n",
        "#             \"operation\": \"import/upload\"\n",
        "#         },\n",
        "#         \"task-1\": {\n",
        "#             \"operation\": \"convert\",\n",
        "#             \"input_format\": \"mp4\",\n",
        "#             \"output_format\": \"wav\",\n",
        "#             \"engine\": \"ffmpeg\",\n",
        "#             \"input\": [\n",
        "#                 \"import-mp4\"\n",
        "#             ],\n",
        "#             \"audio_codec\": \"pcm_s16le\",\n",
        "#             \"audio_bitrate\": 128,\n",
        "#             \"engine_version\": \"5.1.2\"\n",
        "#         }\n",
        "#     },\n",
        "#     \"tag\": \"jobbuilder\"\n",
        "# });"
      ],
      "metadata": {
        "id": "PxFlq0U8txjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0_tup8RAyBy"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "!pip install -q git+https://github.com/openai/whisper.git > /dev/null\n",
        "!pip install -q git+https://github.com/pyannote/pyannote-audio > /dev/null\n",
        "\n",
        "import whisper\n",
        "import datetime\n",
        "\n",
        "import subprocess\n",
        "\n",
        "import torch\n",
        "import pyannote.audio\n",
        "from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\n",
        "embedding_model = PretrainedSpeakerEmbedding(\n",
        "    \"speechbrain/spkrec-ecapa-voxceleb\",\n",
        "    device=torch.device(\"cuda\"))\n",
        "# Model options: dwarkesh/whisper-speaker-recognition, speechbrain/spkrec-ecapa-voxceleb\n",
        "\n",
        "from pyannote.audio import Audio\n",
        "from pyannote.core import Segment\n",
        "\n",
        "import wave\n",
        "# import m4a\n",
        "import contextlib\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiE3hs3jnTlf"
      },
      "outputs": [],
      "source": [
        "# if path[-3:] != 'wav':\n",
        "#   subprocess.call(['ffmpeg', '-i', path, 'audio.wav', '-y'])\n",
        "#   path = 'audio.wav'\n",
        "\n",
        "path = '/content/transcripts/audio.wav'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vdbad9x5CHkC"
      },
      "outputs": [],
      "source": [
        "model = whisper.load_model(model_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4uw8CrovIN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cef387a-e73f-453a-f781-3eeaf233338a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "                                                                          \u001b[A"
          ]
        }
      ],
      "source": [
        "result = model.transcribe(path)\n",
        "segments = result[\"segments\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1sZYZ_pkV1u"
      },
      "outputs": [],
      "source": [
        "with contextlib.closing(wave.open(path,'r')) as f:\n",
        "  frames = f.getnframes()\n",
        "  rate = f.getframerate()\n",
        "  duration = frames / float(rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9R5bpc3_EOL"
      },
      "outputs": [],
      "source": [
        "audio = Audio()\n",
        "\n",
        "def segment_embedding(segment):\n",
        "  start = segment[\"start\"]\n",
        "  # Whisper overshoots the end timestamp in the last segment\n",
        "  end = min(duration, segment[\"end\"])\n",
        "  clip = Segment(start, end)\n",
        "  waveform, sample_rate = audio.crop(path, clip)\n",
        "  return embedding_model(waveform[None])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyannote.audio"
      ],
      "metadata": {
        "id": "OYMoJdb3wLON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyannote.audio. import SequenceEmbedding\n",
        "!pip install librosa\n",
        "\n",
        "# Load the audio data and convert it to mono\n",
        "audio, sample_rate = librosa.load(path, sr=None, mono=True)\n",
        "\n",
        "# Initialize the embedding extractor\n",
        "embedding_extractor = SequenceEmbedding()\n",
        "\n",
        "# Extract the embeddings for each segment of the audio\n",
        "segments = [audio[start:end] for start, end in segment_times]\n",
        "embeddings = np.zeros(shape=(len(segments), 192))\n",
        "for i, segment in enumerate(segments):\n",
        "  embeddings[i] = embedding_extractor(segment)\n",
        "\n",
        "# Replace any NaN values with 0\n",
        "embeddings = np.nan_to_num(embeddings)\n"
      ],
      "metadata": {
        "id": "hqRyf9A4v-Gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPnKe_yQPWkd"
      },
      "outputs": [],
      "source": [
        "embeddings = np.zeros(shape=(len(segments), 192))\n",
        "for i, segment in enumerate(segments):\n",
        "  embeddings[i] = segment_embedding(segment)\n",
        "\n",
        "embeddings = np.nan_to_num(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHvbUf8sgUVA"
      },
      "outputs": [],
      "source": [
        "clustering = AgglomerativeClustering(num_speakers).fit(embeddings)\n",
        "labels = clustering.labels_\n",
        "for i in range(len(segments)):\n",
        "  segments[i][\"speaker\"] = 'SPEAKER ' + str(labels[i] + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4kitnXJLcsX"
      },
      "outputs": [],
      "source": [
        "def time(secs):\n",
        "  return datetime.timedelta(seconds=round(secs))\n",
        "\n",
        "f = open(\"transcript.txt\", \"w\")\n",
        "\n",
        "for (i, segment) in enumerate(segments):\n",
        "  if i == 0 or segments[i - 1][\"speaker\"] != segment[\"speaker\"]:\n",
        "    f.write(\"\\n\" + segment[\"speaker\"] + ' ' + str(time(segment[\"start\"])) + '\\n')\n",
        "  f.write(segment[\"text\"][1:] + ' ')\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fkzO-OTrZKM"
      },
      "source": [
        "# Use Transcription to get Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4oRTADJreXT"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install chromadb\n",
        "!pip install \"unstructured[local-inference]\"\n",
        "!pip install Pillow\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  If using an external .txt file not processed by whisper, run this cell\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "path = next(iter(uploaded))"
      ],
      "metadata": {
        "id": "9GpfVSf2q5K8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMGhdhadsoBw"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "# from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import UnstructuredFileLoader\n",
        "from langchain import OpenAI, PromptTemplate\n",
        "import glob\n",
        "from langchain.llms.openai import OpenAI\n",
        "# from langchain.utils import truncate_text\n",
        "\n",
        "# import glob\n",
        "# from openai import OpenAI, UnstructuredFileLoader, PromptTemplate, load_summarize_chain\n",
        "# from openai.util import truncate_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7QW9XOjTTJk"
      },
      "outputs": [],
      "source": [
        "# llm = OpenAI(temperature=0.2)\n",
        "# def summarize_pdfs_from_folder(pdfs_folder):\n",
        "#     summaries = []\n",
        "#     for pdf_file in glob.glob(pdfs_folder + \"/*.pdf\"):\n",
        "#         loader = PyPDFLoader(pdf_file)\n",
        "#         docs = loader.load_and_split()\n",
        "#         chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "#         summary = chain.run(docs)\n",
        "#         print(\"Summary for: \", pdf_file)\n",
        "#         print(summary)\n",
        "#         print(\"\\n\")\n",
        "#         summaries.append(summary)\n",
        "\n",
        "#     return summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3bbKp0mldP5"
      },
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = 'sk-'\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, PromptTemplate, LLMChain\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains.mapreduce import MapReduceChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "text_splitter = CharacterTextSplitter()"
      ],
      "metadata": {
        "id": "3OYXZTQ9FZMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, PromptTemplate, LLMChain\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains.mapreduce import MapReduceChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "text_splitter = CharacterTextSplitter()\n",
        "transcription_file = \"./transcripts/transcript.txt\"\n",
        "llm = OpenAI(temperature=0, verbose=True)\n",
        "\n",
        "with open(transcription_file, \"r\") as f:\n",
        "        text = f.read()\n",
        "texts = text_splitter.split_text(text)\n",
        "\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "docs = [Document(page_content=t) for t in texts[:3]]\n",
        "\n",
        "CUSTOM_PROMPT = \"\"\"Write a concise summary of the following meeting transcription. Our goals in these meetings are to Identify strategic priorities and define specific tactics and initiatives. Additionally include any information or insight related to those goals in the summary with this structure:\n",
        "Problem being solved;\n",
        "Approach;\n",
        "Main results;\n",
        "Main Discussion Points;\n",
        "List of Topics Discussed;\n",
        "Full concise summary;  \"\"\"\n",
        "\n",
        "prompt_template = CUSTOM_PROMPT + \"\"\"\n",
        "\n",
        "{text}\n",
        "\n",
        "SUMMARY:\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
        "chain = load_summarize_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
        "summary_output = chain.run(docs)\n",
        "\n",
        "with open(\"custom_summary.txt\", \"w\") as f:\n",
        "    f.write(summary_output)"
      ],
      "metadata": {
        "id": "koqXtjmABws8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this if using a single text file that contains the transcription\n",
        "\n",
        "\n",
        "transcription_file = \"./transcripts/transcript.txt\"\n",
        "llm = OpenAI(temperature=0.4, verbose=True)\n",
        "\n",
        "def custom_summary(transcription_file, custom_prompt):\n",
        "    with open(transcription_file, \"r\") as f:\n",
        "        text = f.read()\n",
        "    prompt_template = CUSTOM_PROMPT + \"\"\"\n",
        "\n",
        "    {text}\n",
        "\n",
        "    SUMMARY:\"\"\"\n",
        "    PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
        "    chain = load_summarize_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
        "    chain.run(docs)\n",
        "    return chain\n",
        "\n",
        "CUSTOM_PROMPT = \"\"\"Write a concise summary of the following meeting transcription. Our goals in these meetings are to Identify strategic priorities and define specific tactics and initiatives. Additionally include any information or insight related to those goals in the summary with this structure:\n",
        "Problem being solved;\n",
        "Approach;\n",
        "Main results;\n",
        "Main Discussion Points;\n",
        "List of Topics Discussed;\n",
        "Full concise summary;  \"\"\"\n",
        "\n",
        "custom_summary = custom_summary(transcription_file, custom_prompt=CUSTOM_PROMPT)\n",
        "\n",
        "# Save the summary into a .txt file\n",
        "with open(\"custom_summary.txt\", \"w\") as f:\n",
        "    f.write(custom_summary)\n",
        "\n"
      ],
      "metadata": {
        "id": "myfSyPktuXF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transcriptions = \"./transcripts/\"\n",
        "\n",
        "# llm = OpenAI(temperature=0.4, verbose=True)\n",
        "\n",
        "# def custom_summary(transcriptions, custom_prompt, max_tokens=700):\n",
        "#     summaries = []\n",
        "#     for txt_file in glob.glob(transcriptions + \"*.txt\"):\n",
        "#         with open(txt_file, \"r\") as f:\n",
        "#             text = f.read()\n",
        "#         prompt_template = custom_prompt + \"\"\"\n",
        "\n",
        "#         {text}\n",
        "\n",
        "#         SUMMARY:\"\"\"\n",
        "#         PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
        "#         loader = UnstructuredFileLoader(txt_file)\n",
        "#         docs = loader.load_and_split()\n",
        "#         chain = load_summarize_chain(llm, chain_type=\"map_reduce\",\n",
        "#                                     map_prompt=PROMPT, combine_prompt=PROMPT)\n",
        "#         summary_output = chain({\"input_documents\": docs}, return_only_outputs=True)[\"output_text\"]\n",
        "#         summary_output = truncate_text(summary_output, max_tokens)\n",
        "#         summaries.append(summary_output)\n",
        "\n",
        "#     return summaries\n",
        "\n",
        "# CUSTOM_PROMPT = \"\"\"Write a concise summary of the following meeting transcriptions. Our goals in these meetings are to Identify strategic priorities and define specific tactics and initiatives. Additionally include any information or insight related to those goals in the summary with this structure:\n",
        "# Problem being solved;\n",
        "# Approach;\n",
        "# Main results;\n",
        "# Main Discussion Points;\n",
        "# List of Topics Discussed;\n",
        "# Full concise summary;  \"\"\"\n",
        "\n",
        "# custom_summaries = custom_summary(transcriptions, custom_prompt=CUSTOM_PROMPT)\n",
        "\n",
        "# # Save all summaries into one .txt file\n",
        "# with open(\"custom_summaries.txt\", \"w\") as f:\n",
        "#     for summary in custom_summaries:\n",
        "#         f.write(summary + \"\\n\"*3)"
      ],
      "metadata": {
        "id": "bETz4vcsFN-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqAPaLNfsn5_"
      },
      "outputs": [],
      "source": [
        "# # Run this if using the audio file instead of a precompiled transcription text file\n",
        "# llm = OpenAI(temperature=0.2, verbose=True)\n",
        "\n",
        "# def custom_summary(transcriptions, custom_prompt):\n",
        "#     summaries = []\n",
        "#     for txt_file in glob.glob(transcriptions + \"/*.txt\"):\n",
        "#         loader = UnstructuredFileLoader(txt_file)\n",
        "#         docs = loader.load_and_split()\n",
        "#         prompt_template = custom_prompt + \"\"\"\n",
        "\n",
        "#         {text}\n",
        "\n",
        "#         SUMMARY:\"\"\"\n",
        "#         PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
        "#         chain = load_summarize_chain(llm, chain_type=\"map_reduce\",\n",
        "#                                     map_prompt=PROMPT, combine_prompt=PROMPT)\n",
        "#         summary_output = chain({\"input_documents\": docs}, return_only_outputs=True)[\"output_text\"]\n",
        "#         summaries.append(summary_output)\n",
        "\n",
        "#     return summaries\n",
        "\n",
        "\n",
        "# CUSTOM_PROMPT = \"Write a concise summary of the following meeting transcriptions with this structure: Problem being solved; Approach; Main results; Main Discussion Points; List of Topics Discussed.  Our goals in these meetings are to Identify strategic priorities and define specific tactics and initiatives. Include any information or insight related to those goals in the summary\"\n",
        "# custom_summaries = custom_summary(\"./txt\", custom_prompt=CUSTOM_PROMPT)\n",
        "# # Save all summaries into one .txt file\n",
        "# with open(\"custom_summaries.txt\", \"w\") as f:\n",
        "#     for summary in custom_summaries:\n",
        "#         f.write(summary + \"\\n\"*3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahnEruepSpc2"
      },
      "outputs": [],
      "source": [
        "# Run this if using multiple PDFs\n",
        "\n",
        "# summaries = summarize_pdfs_from_folder(\"./pdfs\")\n",
        "\n",
        "# # Save all summaries into one .txt file\n",
        "# with open(\"summaries.txt\", \"w\") as f:\n",
        "#     for summary in summaries:\n",
        "#         f.write(summary + \"\\n\"*3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BREAK - This is a test"
      ],
      "metadata": {
        "id": "GsWyd9AtFUEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "\n",
        "import whisper\n",
        "import gradio as gr\n",
        "import datetime\n",
        "\n",
        "import subprocess\n",
        "\n",
        "import torch\n",
        "import pyannote.audio\n",
        "from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\n",
        "\n",
        "from pyannote.audio import Audio\n",
        "from pyannote.core import Segment\n",
        "\n",
        "import wave\n",
        "import contextlib\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import numpy as np\n",
        "\n",
        "model = whisper.load_model(\"large-v2\")\n",
        "embedding_model = PretrainedSpeakerEmbedding(\n",
        "    \"speechbrain/spkrec-ecapa-voxceleb\",\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        ")\n",
        "\n",
        "def transcribe(audio, num_speakers):\n",
        "  path, error = convert_to_wav(audio)\n",
        "  if error is not None:\n",
        "    return error\n",
        "\n",
        "  duration = get_duration(path)\n",
        "  if duration > 4 * 60 * 60:\n",
        "    return \"Audio duration too long\"\n",
        "\n",
        "  result = model.transcribe(path)\n",
        "  segments = result[\"segments\"]\n",
        "\n",
        "  num_speakers = min(max(round(num_speakers), 1), len(segments))\n",
        "  if len(segments) == 1:\n",
        "    segments[0]['speaker'] = 'SPEAKER 1'\n",
        "  else:\n",
        "    embeddings = make_embeddings(path, segments, duration)\n",
        "    add_speaker_labels(segments, embeddings, num_speakers)\n",
        "  output = get_output(segments)\n",
        "  return output\n",
        "\n",
        "def convert_to_wav(path):\n",
        "  if path[-3:] != 'wav':\n",
        "    new_path = '.'.join(path.split('.')[:-1]) + '.wav'\n",
        "    try:\n",
        "      subprocess.call(['ffmpeg', '-i', path, new_path, '-y'])\n",
        "    except:\n",
        "      return path, 'Error: Could not convert file to .wav'\n",
        "    path = new_path\n",
        "  return path, None\n",
        "\n",
        "def get_duration(path):\n",
        "  with contextlib.closing(wave.open(path,'r')) as f:\n",
        "    frames = f.getnframes()\n",
        "    rate = f.getframerate()\n",
        "    return frames / float(rate)\n",
        "\n",
        "def make_embeddings(path, segments, duration):\n",
        "  embeddings = np.zeros(shape=(len(segments), 192))\n",
        "  for i, segment in enumerate(segments):\n",
        "    embeddings[i] = segment_embedding(path, segment, duration)\n",
        "  return np.nan_to_num(embeddings)\n",
        "\n",
        "audio = Audio()\n",
        "\n",
        "def segment_embedding(path, segment, duration):\n",
        "  start = segment[\"start\"]\n",
        "  # Whisper overshoots the end timestamp in the last segment\n",
        "  end = min(duration, segment[\"end\"])\n",
        "  clip = Segment(start, end)\n",
        "  waveform, sample_rate = audio.crop(path, clip)\n",
        "  return embedding_model(waveform[None])\n",
        "\n",
        "def add_speaker_labels(segments, embeddings, num_speakers):\n",
        "  clustering = AgglomerativeClustering(num_speakers).fit(embeddings)\n",
        "  labels = clustering.labels_\n",
        "  for i in range(len(segments)):\n",
        "    segments[i][\"speaker\"] = 'SPEAKER ' + str(labels[i] + 1)\n",
        "\n",
        "def time(secs):\n",
        "  return datetime.timedelta(seconds=round(secs))\n",
        "\n",
        "def get_output(segments):\n",
        "  output = ''\n",
        "  for (i, segment) in enumerate(segments):\n",
        "    if i == 0 or segments[i - 1][\"speaker\"] != segment[\"speaker\"]:\n",
        "      if i != 0:\n",
        "        output += '\\n\\n'\n",
        "      output += segment[\"speaker\"] + ' ' + str(time(segment[\"start\"])) + '\\n\\n'\n",
        "    output += segment[\"text\"][1:] + ' '\n",
        "  return output\n",
        "\n",
        "gr.Interface(\n",
        "    title = 'Whisper with Speaker Recognition',\n",
        "    fn=transcribe,\n",
        "    inputs=[\n",
        "        gr.inputs.Audio(source=\"upload\", type=\"filepath\"),\n",
        "        gr.inputs.Number(default=2, label=\"Number of Speakers\")\n",
        "\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.outputs.Textbox(label='Transcript')\n",
        "    ]\n",
        "  ).launch()"
      ],
      "metadata": {
        "id": "Xx7ukkOKFXS5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}