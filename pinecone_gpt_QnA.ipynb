{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHiI4I8tz+m4YARFB/Nrdw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blockchainrelativity/Colab_Experiments/blob/main/pinecone_gpt_QnA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*This Colab is for using document loader to pull in documents from Notion, create embeddings for semantic search, then use a QnA chatbot to query those docs *"
      ],
      "metadata": {
        "id": "vKP0JpZB0Eqx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gdrive to grab the zip file containing the Notion export"
      ],
      "metadata": {
        "id": "vnJApbGEP7EF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHaUudRhMcdU",
        "outputId": "e9776f44-67e4-4b21-ef00-eb5af3e5ddee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/DataBases/cleaned.zip -d Notion_DB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnth4UR5IoiY",
        "outputId": "219576bf-ea4b-424d-f5a0-5d8ead4285f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/DataBases/cleaned.zip\n",
            "replace Notion_DB/5/Archive/Getting Started/Example sub page/columnsteam2.gif? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install packages"
      ],
      "metadata": {
        "id": "95TyqeYbm0OS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install pinecone-client\n",
        "!pip install tiktoken\n",
        "!pip install deeplake\n",
        "!pip install discord\n",
        "!pip install asyncio"
      ],
      "metadata": {
        "id": "tVu8eIFoNC11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load your data"
      ],
      "metadata": {
        "id": "dQRNfA_Qxxla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, NotionDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "dFgMrAicqVVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = NotionDirectoryLoader(\"Notion_DB\")\n",
        "# loader = UnstructuredPDFLoader(\"../data/field-guide-to-data-science.pdf\")\n",
        "# loader = OnlinePDFLoader(\"https://wolfpaulus.com/wp-content/uploads/2017/05/field-guide-to-data-science.pdf\")"
      ],
      "metadata": {
        "id": "ETfJVfaUx2GU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = loader.load()"
      ],
      "metadata": {
        "id": "BxYYkSEjx5xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (f'You have {len(data)} document(s) in your data')\n",
        "print (f'There are {len(data[0].page_content)} characters in your document')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyWd6Lv4x7tn",
        "outputId": "c934ee8d-f9e0-4cfd-b45e-e583c88df533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You have 2506 document(s) in your data\n",
            "There are 1149 characters in your document\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunk your data up into smaller documents"
      ],
      "metadata": {
        "id": "7gonSSxCx-2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "uaKppRicyA6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3602ed4-7820-4a00-cdc7-cf89e2bd68e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/text_splitter.py:57: RuntimeWarning: coroutine 'main' was never awaited\n",
            "  new_doc = Document(\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/text_splitter.py:57: RuntimeWarning: coroutine 'Client.run.<locals>.runner' was never awaited\n",
            "  new_doc = Document(\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (f'Now you have {len(texts)} documents')"
      ],
      "metadata": {
        "id": "QQ2yw5UKyC20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9345e4b-c1f6-4f75-db20-8e6dae807d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now you have 6215 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create embeddings of your documents to get ready for semantic search**"
      ],
      "metadata": {
        "id": "8rVC9c21yEMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.vectorstores import Chroma, Pinecone\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "import pinecone"
      ],
      "metadata": {
        "id": "zJUFd9t3yGbK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b20af22-929d-46f9-d22b-930e201af9eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = ''\n",
        "PINECONE_API_KEY = ''\n",
        "PINECONE_API_ENV = ''"
      ],
      "metadata": {
        "id": "IiV-s-5CyxSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "gC9Za0zmyywN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize pinecone\n",
        "pinecone.init(\n",
        "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
        "    environment=PINECONE_API_ENV  # next to api key in console\n",
        ")\n",
        "index_name = \"lablab-embeds\""
      ],
      "metadata": {
        "id": "asly2xP4y0QN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name)"
      ],
      "metadata": {
        "id": "QY6LLkgNy2Ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18711030-a126-4f7d-ee8b-365991414c8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tiktoken/core.py:50: RuntimeWarning: coroutine 'Client.run.<locals>.runner' was never awaited\n",
            "  self._core_bpe = _tiktoken.CoreBPE(mergeable_ranks, special_tokens, pat_str)\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query those docs to get your answer back"
      ],
      "metadata": {
        "id": "_HXy5Eiay5DO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain"
      ],
      "metadata": {
        "id": "pFYomntpYcH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall langchain\n",
        "!pip install langchain"
      ],
      "metadata": {
        "id": "5c6fX6cLYmRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain"
      ],
      "metadata": {
        "id": "_AUJMBGuy7C7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
        "\n",
        "Current conversation:\n",
        "{history}\n",
        "Friend: {input}\n",
        "AI:\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    input_variables=[\"history\", \"input\"], template=template\n",
        ")"
      ],
      "metadata": {
        "id": "8KDMPr52WNei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.conversation.base import ConversationChain\n",
        "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", verbose=True)\n",
        "# chain = ConversationChain(\n",
        "#     prompt=PROMPT,\n",
        "#     llm=llm,\n",
        "#     verbose=True,\n",
        "#     memory=ConversationBufferMemory(human_prefix=\"Friend\")\n",
        "# )"
      ],
      "metadata": {
        "id": "vU32AMAMy8aM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thinking about how to customize the system prompt"
      ],
      "metadata": {
        "id": "E20vAgvaVMC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"who are you?\"\n",
        "# query = \"System: You are to view the contents of the 'Notes/Meetings/Ideas' as well as the property values and use the linked 'Media' pages as reference as well. Query: Using the 'Tags' property in the 'Notes/Meetings/Ideas' database, look for recent 'DAP Meeting' entries by Created date, and summarize them into a structured report\"\n",
        "# query = \"looking in the action_items database, what are some of the most recent topics?\"\n",
        "docs = docsearch.similarity_search(query, include_metadata=True)"
      ],
      "metadata": {
        "id": "fR4ouVimy9y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out what docs werer selected for this question\n",
        "docs"
      ],
      "metadata": {
        "id": "rwwFZuR46h5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "id": "pIpPs_kay_BZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "outputId": "405a2eff-707e-4767-c210-9933432a693e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Friend: who are you?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-162bc66d5617>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconversation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_documents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_only_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     async def acall(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mprep_outputs\u001b[0;34m(self, inputs, outputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_only_outputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/memory/chat_memory.py\u001b[0m in \u001b[0;36msave_context\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;34m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0minput_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_input_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_user_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_ai_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/memory/chat_memory.py\u001b[0m in \u001b[0;36m_get_input_output\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     ) -> Tuple[str, str]:\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mprompt_input_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prompt_input_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mprompt_input_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/memory/utils.py\u001b[0m in \u001b[0;36mget_prompt_input_key\u001b[0;34m(inputs, memory_variables)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprompt_input_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_variables\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"stop\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_input_keys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"One input key expected got {prompt_input_keys}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprompt_input_keys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: One input key expected got ['input_documents', 'input']"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QnA semantic search"
      ],
      "metadata": {
        "id": "ITqCnU9VqAyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup env"
      ],
      "metadata": {
        "id": "W-8UAMVjnEcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from langchain.vectorstores import DeepLake\n",
        "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import OpenAI\n",
        "import asyncio\n",
        "\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "M4a007KnnJIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = \"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\"\n",
        "ACTIVELOOP_TOKEN = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTY4MzAwMjU0NywiZXhwIjoxNjg0Mjk4NTIwfQ.eyJpZCI6ImJsb2NrY2hhaW5yZWxhdGl2aXR5In0.fd0nCMqFsGo1JOYQZrC67tLwHOPDpE_LLOGO4TdxXsMQOrede935L01yO6Y7GyyxkIi9obJPBSaHf-8lfb3AOQ\"\n",
        "ACTIVELOOP_ORG = \"blockchainrelativity\""
      ],
      "metadata": {
        "id": "iv9tJO-Nx4qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')\n",
        "os.environ['ACTIVELOOP_TOKEN'] = getpass.getpass('Activeloop Token:')\n",
        "os.environ['ACTIVELOOP_ORG'] = getpass.getpass('Activeloop Org:')\n",
        "\n",
        "org = os.environ['ACTIVELOOP_ORG']\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "dataset_path = 'hub://' + org + '/data'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EuTrYLJqhVF",
        "outputId": "6cafbbbf-6164-45c3-a2aa-df9f2a1144da"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n",
            "Activeloop Token:··········\n",
            "Activeloop Org:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grab conversation and write to messages.txt"
      ],
      "metadata": {
        "id": "NgJE0PKYrmoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is using Discord bot to scrape the messages in all channels. It seems to be working but expected behavior does not occur. writes nothing to file\n",
        "Discord_Token = \"\"\n",
        "Server_ID = \"\"\n",
        "\n",
        "import discord\n",
        "import asyncio\n",
        "\n",
        "intents = discord.Intents.default()\n",
        "intents.message_content = True\n",
        "\n",
        "client = discord.Client(intents=intents)\n",
        "\n",
        "async def scrape_channels():\n",
        "    # Replace YOUR_SERVER_ID_HERE with your server ID\n",
        "    guild = await client.fetch_guild(Server_ID)\n",
        "    print(f\"Bot has connected to {guild.name} ({guild.id})\")\n",
        "\n",
        "    with open(\"messages.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for channel in guild.text_channels:\n",
        "            print(f\"Scraping messages from {channel.name} ({channel.id})\")\n",
        "            async for message in channel.history(limit=None):\n",
        "                # Write the message content to the file\n",
        "                f.write(f\"{message.author.name}: {message.content}\\n\")\n",
        "        print(\"Scraping complete!\")\n",
        "\n",
        "    # Close the file\n",
        "    f.close()\n",
        "\n",
        "@client.event\n",
        "async def on_ready():\n",
        "    print(f\"Logged in as {client.user}\")\n",
        "\n",
        "    # Schedule the scraping task\n",
        "    asyncio.create_task(scrape_channels())\n",
        "\n",
        "@client.event\n",
        "async def on_message(message):\n",
        "    await scrape_channels()\n",
        "\n",
        "# Replace YOUR_TOKEN_HERE with your Discord bot token\n",
        "client.run(Discord_Token)\n"
      ],
      "metadata": {
        "id": "QxgtKkx6Eu61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xhqDx7ROPAGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ingest chat embeddings"
      ],
      "metadata": {
        "id": "krh6AkoOsDzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"messages.txt\") as f:\n",
        "    state_of_the_union = f.read()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "pages = text_splitter.split_text(state_of_the_union)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "texts = text_splitter.split_documents(pages)\n",
        "\n",
        "print (texts)\n",
        "\n",
        "dataset_path = 'hub://'+org+'/data'\n",
        "embeddings = OpenAIEmbeddings()\n",
        "db = DeepLake.from_documents(texts, embeddings, dataset_path=dataset_path, overwrite=True)"
      ],
      "metadata": {
        "id": "Jdy-7K62roCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ask questions"
      ],
      "metadata": {
        "id": "X619KBBvsLb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db = DeepLake(dataset_path=dataset_path, read_only=True, embedding_function=embeddings)\n",
        "\n",
        "retriever = db.as_retriever()\n",
        "retriever.search_kwargs['distance_metric'] = 'cos'\n",
        "retriever.search_kwargs['k'] = 4\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever, return_source_documents=False)\n",
        "\n",
        "# What was the restaurant the group was talking about called?\n",
        "query = input(\"Enter query:\")\n",
        "\n",
        "# The Hungry Lobster\n",
        "ans = qa({\"query\": query})\n",
        "\n",
        "print(ans)"
      ],
      "metadata": {
        "id": "JmjUJaVQrn-F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}